# Por Qu√© Mis Alertas del Homelab Fallaban Silenciosamente (Y C√≥mo Lo Solucion√©)

*Una historia de debugging sobre autenticaci√≥n, webhooks y c√≥mo hacer que el monitoreo realmente funcione*

---

Pas√© semanas pensando que el monitoreo de mi homelab funcionaba perfectamente. Prometheus recolectaba m√©tricas, Alertmanager estaba configurado, y ten√≠a ntfy configurado para enviar notificaciones push a mi celular. El dashboard mostraba que todo estaba saludable.

Un d√≠a, not√© que mi PostgreSQL hab√≠a estado ca√≠do por 3 horas. Ninguna notificaci√≥n. Nada en mi celular. La alerta se hab√≠a disparado ‚Äî pod√≠a verla en Grafana ‚Äî pero la notificaci√≥n nunca me lleg√≥.

Esta es la historia de c√≥mo lo debugge√© y solucion√©.

## El Setup

Mi homelab corre en un cluster K3s de 4 nodos. Para monitoreo, uso el Helm chart kube-prometheus-stack, que me da Prometheus, Grafana y Alertmanager listos para usar. Para notificaciones push, hosteo ntfy ‚Äî un servicio simple de notificaciones pub-sub que tiene apps para iOS y Android.

La arquitectura se ve as√≠:

```mermaid
flowchart LR
    subgraph Cluster K3s
        P[Prometheus] -->|dispara alerta| A[Alertmanager]
        A -->|webhook POST| N[ntfy]
    end
    N -->|notificaci√≥n push| M[üì± App M√≥vil]

    style P fill:#e6522c,color:#fff
    style A fill:#e6522c,color:#fff
    style N fill:#317f6f,color:#fff
    style M fill:#4a90d9,color:#fff
```

Simple, ¬øno? Prometheus detecta un problema, dispara una alerta a Alertmanager, que env√≠a un webhook a ntfy, que pushea a mi celular. ¬øQu√© podr√≠a salir mal?

## El S√≠ntoma

Empec√© chequeando lo obvio. ¬øEstaba ntfy corriendo? S√≠. ¬øAlertmanager estaba configurado correctamente? La config se ve√≠a bien:

```yaml
receivers:
  - name: ntfy-homelab
    webhook_configs:
      - url: http://ntfy.ntfy.svc.cluster.local/homelab-alerts
        send_resolved: true
```

Prob√© enviar un mensaje de prueba directamente a ntfy desde dentro del cluster:

```bash
kubectl exec -n ntfy deployment/ntfy -- \
  curl -d "Mensaje de prueba" http://localhost/homelab-alerts
```

Y obtuve esta respuesta:

```
{"code":40301,"http":403,"error":"forbidden"}
```

403 Forbidden. Pero esper√° ‚Äî pod√≠a suscribirme al topic desde mi celular y recibir mensajes cuando los enviaba desde fuera del cluster. ¬øPor qu√© el request interno estaba siendo rechazado?

## Encontrando la Causa Ra√≠z

Revis√© la configuraci√≥n del servidor ntfy:

```bash
kubectl exec -n ntfy deployment/ntfy -- \
  cat /etc/ntfy/server.yml | grep auth
```

Output:

```
auth-default-access: "read-only"
```

Ah√≠ estaba. Hab√≠a configurado ntfy con `auth-default-access: "read-only"` por seguridad ‚Äî usuarios an√≥nimos pueden suscribirse a topics, pero no pueden publicar. Este es un buen default para un servicio de notificaciones expuesto p√∫blicamente.

Pero Alertmanager estaba enviando webhooks como usuario an√≥nimo. Sin headers de autenticaci√≥n, sin credenciales. El request estaba siendo rechazado antes de que ntfy siquiera mirara el contenido del mensaje.

```mermaid
sequenceDiagram
    participant A as Alertmanager
    participant N as ntfy
    participant M as App M√≥vil

    A->>N: POST /homelab-alerts (sin auth)
    N-->>A: 403 Forbidden ‚ùå
    Note over A,N: Publicaci√≥n an√≥nima bloqueada!

    A->>N: POST /homelab-alerts (con Basic Auth)
    N-->>A: 200 OK ‚úÖ
    N->>M: Notificaci√≥n push
    Note over N,M: Mensaje entregado!
```

## La Soluci√≥n

La soluci√≥n requer√≠a tres partes:

**Parte 1: Crear credenciales para Alertmanager**

Necesitaba darle a Alertmanager un usuario y contrase√±a para autenticarse con ntfy. Siguiendo el principio de menor privilegio, cre√© un usuario dedicado `alertmanager` en lugar de usar el usuario `admin` ‚Äî as√≠ el usuario solo tiene permisos para escribir en el topic `homelab-alerts`, nada m√°s:

```bash
# Generar una contrase√±a segura
PASSWORD=$(openssl rand -base64 24)

# Crear usuario dedicado para Alertmanager
kubectl exec -n ntfy deployment/ntfy -- \
  sh -c "printf '%s\n%s\n' '${PASSWORD}' '${PASSWORD}' | ntfy user add alertmanager"

# Otorgar permisos SOLO al topic homelab-alerts (write-only)
kubectl exec -n ntfy deployment/ntfy -- \
  ntfy access alertmanager homelab-alerts write
```

Este enfoque tiene una ventaja de seguridad: si las credenciales se comprometen, el atacante solo puede publicar en un topic espec√≠fico, no tiene acceso admin a todo ntfy.

**Parte 2: Guardar la contrase√±a como Kubernetes Secret**

Alertmanager puede leer credenciales desde archivos montados como secrets. Cre√© el secret en el namespace monitoring:

```bash
kubectl create secret generic ntfy-alertmanager-password \
  -n monitoring \
  --from-literal=password="${PASSWORD}"
```

**Parte 3: Configurar Alertmanager para usar Basic Auth**

Este fue el cambio clave en mi configuraci√≥n de Terraform. Necesitaba:
1. Montar el secret en el pod de Alertmanager
2. Configurar el receiver del webhook para usar Basic Auth

```
+--------------------------------------------------+
| alertmanager.alertmanagerSpec                    |
+--------------------------------------------------+
| secrets = ["ntfy-alertmanager-password"]         |
|                                                  |
| Esto monta el secret en:                         |
| /etc/alertmanager/secrets/ntfy-alertmanager-     |
| password/password                                |
+--------------------------------------------------+

+--------------------------------------------------+
| webhook_configs                                  |
+--------------------------------------------------+
| url: http://ntfy.ntfy.svc.cluster.local/         |
|      homelab-alerts?template=alertmanager        |
|                                                  |
| http_config:                                     |
|   basic_auth:                                    |
|     username: alertmanager                       |
|     password_file: /etc/alertmanager/secrets/    |
|                    ntfy-alertmanager-password/   |
|                    password                      |
+--------------------------------------------------+
```

Not√° el par√°metro `?template=alertmanager` ‚Äî esto le dice a ntfy que parsee el payload JSON entrante usando su template built-in de Alertmanager, que formatea la notificaci√≥n de forma amigable para m√≥viles.

## Verificando la Soluci√≥n

Despu√©s de aplicar los cambios, revis√© los logs de ntfy:

```bash
kubectl logs -n ntfy deployment/ntfy --tail=10 | grep messages_published
```

```
+----------------------------------------------------------+
| Hora                 | messages_published | Cambio       |
+----------------------------------------------------------+
| 2025-12-31 14:35:18  | 15                 | l√≠nea base   |
| 2025-12-31 14:40:18  | 34                 | +19          |
| 2025-12-31 14:41:18  | 43                 | +9           |
| 2025-12-31 14:46:18  | 48                 | +5           |
+----------------------------------------------------------+
```

¬°El contador de mensajes estaba aumentando! Las alertas finalmente se estaban entregando.

Cre√© un PrometheusRule de prueba para disparar una alerta real:

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: test-alert
  namespace: monitoring
spec:
  groups:
    - name: test
      rules:
        - alert: TestAlertForNtfy
          expr: vector(1)
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: "Alerta de prueba para integraci√≥n ntfy"
```

En 30 segundos, mi celular vibr√≥. La notificaci√≥n mostraba el nombre de la alerta, severidad y descripci√≥n ‚Äî exactamente lo que necesitaba.

## Bonus: Agregando un Widget de Alertas a Homepage

Mientras arreglaba las notificaciones, me di cuenta que tambi√©n quer√≠a un indicador visual r√°pido en mi dashboard de Homepage. No quer√≠a abrir Grafana solo para ver si hab√≠a alertas dispar√°ndose.

Homepage soporta un widget `prometheusmetric` que puede ejecutar queries PromQL directamente. Agregu√© un nuevo widget a mi secci√≥n de Cluster Health:

```yaml
- Cluster Health:
    - Cluster Alerts:
        icon: mdi-alert-circle
        href: https://grafana.chocolandiadc.com/alerting/groups
        description: "Alertas activas de Prometheus"
        widget:
          type: prometheusmetric
          url: http://kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090
          refreshInterval: 30000
          metrics:
            - label: Critical
              query: count(ALERTS{alertstate="firing", severity="critical"}) or vector(0)
              format:
                type: number
            - label: Warning
              query: count(ALERTS{alertstate="firing", severity="warning"}) or vector(0)
              format:
                type: number
```

El `or vector(0)` es importante ‚Äî sin esto, el query no retorna datos cuando hay cero alertas, y el widget muestra un error. Con esto, obtengo un "0" limpio.

```mermaid
flowchart TB
    subgraph Dashboard Homepage
        H[Homepage] -->|query PromQL| P[Prometheus]
        P -->|conteo alertas| W[Widget Cluster Alerts]
    end

    W --> D{Mostrar}
    D -->|Critical > 0| R[üî¥ Critical: N]
    D -->|Warning > 0| Y[üü° Warning: N]
    D -->|Todo OK| G[‚úÖ Critical: 0, Warning: 0]

    style H fill:#4a90d9,color:#fff
    style P fill:#e6522c,color:#fff
    style W fill:#317f6f,color:#fff
```

Ahora puedo mirar mi dashboard e inmediatamente ver el estado de salud del cluster.

## Lecciones Aprendidas

```
+---------------------------------------------------------------+
| Lecci√≥n                         | Aprendizaje                 |
+---------------------------------------------------------------+
| Testear el path completo de     | Los unit tests no alcanzan  |
| notificaciones                  | ‚Äî hay que testear end-to-end|
+---------------------------------------------------------------+
| Los defaults de seguridad       | auth-default-access era     |
| pueden romper integraciones     | correcto, pero olvid√©       |
|                                 | configurar la integraci√≥n   |
+---------------------------------------------------------------+
| Los logs cuentan la historia    | La m√©trica messages_published|
|                                 | de ntfy hizo f√°cil debuggear|
+---------------------------------------------------------------+
| Los errores 403 necesitan       | No asumas que "forbidden"   |
| investigaci√≥n                   | significa "contrase√±a mal"  |
+---------------------------------------------------------------+
```

La iron√≠a no se me escapa ‚Äî mi sistema de monitoreo no se estaba monitoreando a s√≠ mismo. Ten√≠a un punto ciego en el pipeline de notificaciones que podr√≠a haberme costado horas de downtime si algo cr√≠tico hubiera fallado.

Si est√°s corriendo un setup similar, te recomiendo testear tu path completo de alertas regularmente. Cre√° una alerta de prueba, verific√° que llegue a tu celular, y borrala. Toma 30 segundos y puede salvarte de descubrir que tus notificaciones est√°n rotas en el peor momento posible.

## Referencia R√°pida

Para cualquiera implementando esto en su propio homelab, ac√° va un resumen:

```
+---------------------------------------------------------------+
| Componente             | Configuraci√≥n                        |
+---------------------------------------------------------------+
| Modo auth de ntfy      | auth-default-access: "read-only"     |
| Usuario Alertmanager   | alertmanager (write-only homelab-alerts) |
| Storage de password    | K8s Secret en namespace monitoring   |
| Path de mount del secret| /etc/alertmanager/secrets/<nombre>/ |
| Par√°metro URL webhook  | ?template=alertmanager               |
+---------------------------------------------------------------+
```

La implementaci√≥n completa est√° en mi repositorio de homelab bajo el feature `026-ntfy-homepage-alerts`.

---

*¬øTe encontraste con problemas similares de "falla silenciosa" en tu homelab? Me encantar√≠a escuchar tus historias de debugging en los comentarios.*
