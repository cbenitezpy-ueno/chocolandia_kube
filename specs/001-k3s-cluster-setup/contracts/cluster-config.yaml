# Cluster Configuration Contract: ChocolandiaDC K3s HA Cluster
# This file defines the expected configuration schema and validation rules
# for the K3s cluster. It serves as documentation and reference for Terraform implementation.

---
apiVersion: v1
kind: ConfigContract
metadata:
  name: chocolandiadc-cluster-config
  version: 1.0.0
  created: 2025-11-08

cluster:
  # Cluster identification
  name: "chocolandiadc"
  description: "4-node K3s HA cluster for learning Kubernetes and infrastructure automation"

  # K3s configuration
  k3s:
    version: "v1.28.3+k3s1"  # Update to latest stable as needed
    distribution: "k3s"
    ha_mode: true
    embedded_etcd: true

  # Topology
  topology:
    control_plane_count: 3  # Required for HA (etcd quorum)
    worker_count: 1         # Expandable to 3 per spec
    total_nodes: 4

# Control-plane node definitions
control_plane_nodes:
  - hostname: "master1"
    role: "control-plane"
    is_first_node: true  # Bootstrap node
    ip_address: "REPLACE_WITH_ACTUAL_IP"  # Example: 192.168.1.101
    ssh:
      user: "REPLACE_WITH_SSH_USER"        # Example: ubuntu
      key_path: "~/.ssh/id_rsa"
    k3s_flags:
      - "--cluster-init"
      - "--tls-san={{ ip_address }}"
      - "--node-name={{ hostname }}"
    required_resources:
      cpu_cores: 2
      memory_gb: 4
      disk_gb: 20

  - hostname: "master2"
    role: "control-plane"
    is_first_node: false
    ip_address: "REPLACE_WITH_ACTUAL_IP"  # Example: 192.168.1.102
    ssh:
      user: "REPLACE_WITH_SSH_USER"
      key_path: "~/.ssh/id_rsa"
    k3s_flags:
      - "--server=https://{{ master1_ip }}:6443"
      - "--token={{ cluster_token }}"
      - "--node-name={{ hostname }}"
    depends_on:
      - "master1"

  - hostname: "master3"
    role: "control-plane"
    is_first_node: false
    ip_address: "REPLACE_WITH_ACTUAL_IP"  # Example: 192.168.1.103
    ssh:
      user: "REPLACE_WITH_SSH_USER"
      key_path: "~/.ssh/id_rsa"
    k3s_flags:
      - "--server=https://{{ master1_ip }}:6443"
      - "--token={{ cluster_token }}"
      - "--node-name={{ hostname }}"
    depends_on:
      - "master1"

# Worker node definitions
worker_nodes:
  - hostname: "nodo1"
    role: "worker"
    ip_address: "REPLACE_WITH_ACTUAL_IP"  # Example: 192.168.1.104
    ssh:
      user: "REPLACE_WITH_SSH_USER"
      key_path: "~/.ssh/id_rsa"
    k3s_flags:
      - "--server=https://{{ master1_ip }}:6443"
      - "--token={{ cluster_token }}"
      - "--node-name={{ hostname }}"
    depends_on:
      - "master1"
      - "master2"
      - "master3"
    required_resources:
      cpu_cores: 2
      memory_gb: 4
      disk_gb: 20

# Etcd configuration (embedded in control-plane nodes)
etcd:
  mode: "embedded"  # K3s embedded etcd
  members: 3        # master1, master2, master3
  quorum_size: 2    # (3/2)+1 = 2
  failure_tolerance: 1

# Monitoring stack configuration
monitoring:
  namespace: "monitoring"
  enabled: true

  prometheus:
    helm_chart: "kube-prometheus-stack"
    chart_version: "51.0.0"  # Update to latest stable as needed
    release_name: "kube-prometheus-stack"
    retention: "15d"
    storage_size: "10Gi"
    scrape_interval: "30s"

    # Targets to scrape
    targets:
      - kubelet (all nodes)
      - apiserver (control-plane nodes)
      - scheduler (control-plane nodes)
      - controller-manager (control-plane nodes)
      - etcd (control-plane nodes)
      - node-exporter (all nodes)
      - kube-state-metrics

    # Resource limits
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1000m"

  grafana:
    admin_user: "admin"
    admin_password: "AUTO_GENERATED"  # Retrieved from Kubernetes Secret after deployment

    # Pre-configured dashboards
    dashboards:
      - "Kubernetes / Cluster Overview"
      - "Kubernetes / Nodes"
      - "Kubernetes / Pods"
      - "Etcd"
      - "Node Exporter / Nodes"

    # Resource limits
    resources:
      requests:
        memory: "200Mi"
        cpu: "100m"
      limits:
        memory: "500Mi"
        cpu: "500m"

# Network configuration
network:
  pod_cidr: "10.42.0.0/16"      # K3s default
  service_cidr: "10.43.0.0/16"  # K3s default
  cni: "flannel"                # K3s default CNI

# Storage configuration
storage:
  provisioner: "local-path"  # K3s default local storage provisioner
  reclaim_policy: "Delete"
  future_enhancement:
    - "Longhorn for distributed block storage"

# Security configuration
security:
  rbac_enabled: true
  pod_security_standards: "restricted"  # Future enhancement
  network_policies: false               # Future enhancement (requires Calico)
  secrets_encryption_at_rest: true      # etcd encryption enabled

  # Secret management
  secrets:
    cluster_token:
      location: "/var/lib/rancher/k3s/server/node-token"
      retrieval: "SSH from master1 after bootstrap"
      storage: "Terraform state (sensitive)"
    kubeconfig:
      location: "/etc/rancher/k3s/k3s.yaml"
      retrieval: "SSH from master1 after bootstrap"
      storage: "Local file ~/.kube/config (0600 permissions)"
    grafana_password:
      location: "Kubernetes Secret: monitoring/kube-prometheus-stack-grafana"
      retrieval: "kubectl get secret -n monitoring kube-prometheus-stack-grafana -o jsonpath='{.data.admin-password}' | base64 -d"

# Terraform state configuration
terraform:
  backend: "local"
  state_file: "terraform/environments/chocolandiadc/terraform.tfstate"
  backup_strategy:
    enabled: true
    location: "terraform/environments/chocolandiadc/backups/"
    format: "terraform.tfstate.backup.<timestamp>"
  future_enhancement:
    - "S3 backend with state locking (DynamoDB)"
    - "Terraform Cloud"

# Validation rules
validation:
  cluster_name:
    pattern: "^[a-z0-9-]+$"
    max_length: 63
    description: "DNS-compliant cluster name"

  node_hostname:
    control_plane_pattern: "^master[1-3]$"
    worker_pattern: "^nodo[1-3]$"
    description: "Hostnames must match naming convention"

  ip_address:
    type: "IPv4"
    static_required: true
    same_subnet: true
    description: "All nodes must have static IPs on same subnet"

  k3s_version:
    pattern: "^v[0-9]+\\.[0-9]+\\.[0-9]+\\+k3s[0-9]+$"
    example: "v1.28.3+k3s1"
    description: "Valid K3s release version"

  resources:
    min_cpu_cores: 2
    min_memory_gb: 4
    min_disk_gb: 20
    description: "Minimum hardware requirements per node"

# Expected outputs
outputs:
  cluster_name:
    value: "chocolandiadc"
    sensitive: false

  api_endpoint:
    value: "https://<master1_ip>:6443"
    sensitive: false
    description: "Kubernetes API endpoint"

  kubeconfig_path:
    value: "~/.kube/config or terraform/environments/chocolandiadc/kubeconfig"
    sensitive: true
    description: "Path to kubeconfig file"

  cluster_token:
    value: "<auto-generated>"
    sensitive: true
    description: "K3s cluster join token"

  grafana_admin_password:
    value: "<auto-generated>"
    sensitive: true
    description: "Grafana admin password"

  prometheus_url:
    value: "http://<prometheus-service>:9090"
    sensitive: false
    description: "Prometheus web UI (use kubectl port-forward)"

  grafana_url:
    value: "http://<grafana-service>:80"
    sensitive: false
    description: "Grafana web UI (use kubectl port-forward)"

# Testing requirements
testing:
  pre_apply:
    - "terraform fmt -check"
    - "terraform validate"
    - "terraform plan -out=tfplan"

  post_apply_phase1:  # After master1 bootstrap
    - "kubectl get nodes (master1 Ready)"
    - "kubectl get pods -n kube-system (all system pods Running)"

  post_apply_phase2:  # After all control-plane nodes
    - "kubectl get nodes (master1-3 Ready)"
    - "etcd quorum check (3/3 members)"

  post_apply_phase3:  # After worker nodes
    - "kubectl get nodes (all 4 nodes Ready)"
    - "Deploy test pod (nginx)"

  post_apply_phase4:  # After monitoring stack
    - "Prometheus targets check (all up)"
    - "Grafana health check (HTTP 200)"
    - "Dashboard access test"

  ha_failover:
    - "Shutdown master1"
    - "kubectl get nodes (API still accessible)"
    - "Verify etcd quorum (2/3 members)"
    - "Restart master1"
    - "Verify master1 rejoins (3/3 members)"

# Documentation requirements
documentation:
  required_files:
    - "docs/runbooks/cluster-bootstrap.md"
    - "docs/runbooks/adding-nodes.md"
    - "docs/runbooks/disaster-recovery.md"
    - "docs/runbooks/troubleshooting.md"
    - "docs/adrs/001-terraform-over-ansible.md"
    - "docs/adrs/002-k3s-over-k8s.md"
    - "docs/adrs/003-3plus1-topology.md"
    - "docs/adrs/004-prometheus-grafana-stack.md"
    - "terraform/README.md"
    - "terraform/modules/k3s-cluster/README.md"
    - "terraform/modules/k3s-node/README.md"
    - "terraform/modules/monitoring-stack/README.md"
