# Rollback Procedure: Monitoring Stack

**Feature**: 021-monitoring-stack-upgrade
**Date**: 2025-12-27
**Current Version**: 68.4.0
**Previous Version**: 55.5.0

## Overview

This document describes how to rollback kube-prometheus-stack from version 68.4.0 to 55.5.0 if issues are encountered.

## Prerequisites

- kubectl access to the cluster
- Helm CLI available
- Access to OpenTofu/Terraform state

## Rollback Methods

### Method 1: Helm Rollback (Recommended for Quick Recovery)

```bash
# Set kubeconfig
export KUBECONFIG=/Users/cbenitez/chocolandia_kube/terraform/environments/chocolandiadc-mvp/kubeconfig

# View available revisions
helm history kube-prometheus-stack -n monitoring

# Rollback to last working version (revision 15 = v55.5.0)
helm rollback kube-prometheus-stack 15 -n monitoring

# Monitor the rollback
kubectl get pods -n monitoring -w

# Verify version after rollback
helm list -n monitoring
```

### Method 2: OpenTofu Rollback (Recommended for GitOps Consistency)

1. **Revert the version in monitoring.tf:**

```hcl
locals {
  prometheus_stack_version = "55.5.0"  # Reverted from 68.4.0
}
```

2. **Apply the change:**

```bash
cd /Users/cbenitez/chocolandia_kube/terraform/environments/chocolandiadc-mvp

TF_VAR_github_token="dummy" \
TF_VAR_github_app_id="dummy" \
TF_VAR_github_app_installation_id="dummy" \
TF_VAR_github_app_private_key="dummy" \
TF_VAR_govee_api_key="dummy" \
tofu apply -target=helm_release.kube_prometheus_stack
```

## PVC Preservation

**IMPORTANT**: PersistentVolumeClaims (PVCs) are NOT deleted during rollback:

| PVC | Data | Preservation |
|-----|------|--------------|
| prometheus-kube-prometheus-stack-prometheus-db-* | TSDB metrics (15d) | Preserved |
| kube-prometheus-stack-grafana | Grafana SQLite DB | Preserved |

Historical metrics and Grafana settings will remain available after rollback.

## Post-Rollback Verification

After rollback, verify:

```bash
# 1. Check version
helm list -n monitoring
# Expected: Chart version 55.5.0

# 2. Check all pods running
kubectl get pods -n monitoring
# Expected: All pods Running

# 3. Verify retention
kubectl get prometheus -n monitoring kube-prometheus-stack-prometheus -o jsonpath='{.spec.retention}'
# Expected: 15d

# 4. Verify Grafana NodePort
kubectl get svc kube-prometheus-stack-grafana -n monitoring -o jsonpath='{.spec.ports[0].nodePort}'
# Expected: 30000

# 5. Verify Grafana accessible
curl -s http://192.168.4.101:30000/api/health | jq
# Expected: {"database":"ok",...}

# 6. Verify alerts working
kubectl port-forward svc/kube-prometheus-stack-alertmanager 9093:9093 -n monitoring &
sleep 3
curl -X POST http://localhost:9093/api/v2/alerts -H "Content-Type: application/json" \
  -d '[{"labels":{"alertname":"RollbackTestAlert","severity":"info"}}]'
# Check Ntfy for alert
```

## Known Issues During Rollback

### Issue 1: PVC Spec Immutable Error

If you see:
```
PersistentVolumeClaim "kube-prometheus-stack-grafana" is invalid: spec: Forbidden: spec is immutable
```

**Solution**: The PVC already exists and doesn't need modification. The rollback will still work; this error can be ignored if pods eventually start.

### Issue 2: Node Exporter Port Conflict

If node-exporter pods fail to schedule:

```bash
# Check events
kubectl describe pod -n monitoring -l app.kubernetes.io/name=prometheus-node-exporter
```

**Solution**: Ensure `hostNetwork: false` is explicitly set in values.

## Revision Reference

| Revision | Version | Status | Notes |
|----------|---------|--------|-------|
| 15 | 55.5.0 | Previous working | Safe rollback target |
| 16 | 68.4.0 | Current | Upgraded 2025-12-27 |

## Contact

If rollback issues persist, escalate to:
- Review Helm history: `helm history kube-prometheus-stack -n monitoring`
- Check pod events: `kubectl describe pods -n monitoring`
- Review monitoring.tf configuration

---

**Document generated by**: 021-monitoring-stack-upgrade implementation
